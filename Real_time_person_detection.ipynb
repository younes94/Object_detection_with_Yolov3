{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# person detection from image, video with Yolov3 et openCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Object detection from image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Télechargements des poids de Yolov3 et du fichier de configuration de Yolov3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement  de Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet(\"path/yolov3.weights\", \"yolov3.cfg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Récupération des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "classes = []\n",
    "with open('coco.names') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('Path/image.jpeg')\n",
    "#print(img.shape)\n",
    "height, width, channels = img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Passer l'image dans le réseau et de faire la détection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outs est un tableau qui contient toutes les informations sur les objets détectés, leur position et le degré de confiance de la détection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = cv2.dnn.blobFromImage(img, 1/255, (416,416), (0,0,0), True, crop = False)\n",
    "net.setInput(blob)   \n",
    "outputLayerNames = net.getUnconnectedOutLayersNames()\n",
    "outs = net.forward(outputLayerNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À ce stade, la détection est faite, et il suffit d'afficher le résultat à l'écran.\n",
    "<br/>on passe ensuite en boucle dans le tableau des résultats, on calcule la confiance et on choisit un seuil de confiance.\n",
    "<br/>on fixe un seuil de confiance de 0,5, s'il est supérieur, on considère que l'objet est correctement détecté, sinon on le saute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            # Object detected\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "            # Rectangle coordinates\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "            boxes.append([x, y, w, h])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'affichages des boites avec l'application d'une fonction pour supprimer ce lebruit \"NMSBoxe\".\n",
    "<br/>Si l'objet est une personne on l'affiche sinn on l'affiche pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Image was successfully saved\n"
     ]
    }
   ],
   "source": [
    "indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "for i in range(len(boxes)):\n",
    "    if i in indexes:\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = str(classes[class_ids[i]])\n",
    "        if str(classes[class_ids[i]]) == 'person':\n",
    "            label = str(classes[class_ids[i]])\n",
    "            confidence = str(round(confidences[i],2))\n",
    "            color = colors[i]\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(img, label +\" \"+confidence, (x, y + 30), font, 2, color, 2)\n",
    "cv2.imshow(\"Image\", img)\n",
    "\n",
    "\n",
    "\n",
    "cv2.imwrite('Person_detection.jpg', img) \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(\"The Image was successfully saved\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Object detection from video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Le meme principe utilisé pour l'image c-a-d la détection se fait frame par frame sur un video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video was successfully saved\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"path/xxxx.mp4\")\n",
    "frame_width = int(cap.get(3)) \n",
    "frame_height = int(cap.get(4)) \n",
    "   \n",
    "size = (frame_width, frame_height) \n",
    "   \n",
    "# Stockage de resulats en videos sous le nom de person_detection. \n",
    "result = cv2.VideoWriter('person_detection_video.avi',  \n",
    "                         cv2.VideoWriter_fourcc(*'MJPG'), \n",
    "                         10, size) \n",
    "while(True):\n",
    "    ret, img = cap.read()\n",
    "    if ret ==True:\n",
    "    \n",
    "        height, width, channels = img.shape\n",
    "        if ret:\n",
    "            assert not isinstance(img,type(None)), 'frame not found'\n",
    "        blob = cv2.dnn.blobFromImage(img, 1/255, (416,416), (0,0,0), True, crop = False)\n",
    "        net.setInput(blob)    \n",
    "        outputLayerNames = net.getUnconnectedOutLayersNames()\n",
    "        outs = net.forward(outputLayerNames)\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5:\n",
    "                # Object detected\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "                    # Rectangle coordinates\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "        colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_PLAIN\n",
    "        for i in range(len(boxes)):\n",
    "            if i in indexes:\n",
    "                x, y, w, h = boxes[i]\n",
    "               \n",
    "                label = str(classes[class_ids[i]])\n",
    "                confidence = str(round(confidences[i],2))\n",
    "                color = colors[i]\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(img, label +\" \"+confidence, (x, y + 30), font, 2, color, 2)\n",
    "\n",
    "\n",
    "        result.write(img)\n",
    "\n",
    "        cv2.imshow('Recording...', img)\n",
    "\n",
    "\n",
    "\n",
    "        if(cv2.waitKey(1) & 0xFF==ord('q')):\n",
    "            break\n",
    "    else: \n",
    "        break\n",
    "cap.release() \n",
    "result.release() \n",
    "    \n",
    "cv2.destroyAllWindows() \n",
    "print(\"The video was successfully saved\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
